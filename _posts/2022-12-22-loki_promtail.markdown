---
layout: single
title:  "An untapped data source: Loki and Promtail (under construction)"
date:   2022-12-22 08:00:00
comments: true
categories: Loki, Promtail, Grafana, data, log
---

## Motivation

For years, I have been using numerous different databases on an almost daily basis. Oracle, PostgreSQL, MariaDB,
InfluxDB, etc.. etc.. I enjoy the challenge of writing complex queries and discovering patterns or trends in the results. 
I believe that most individuals in a business setting should be familiar with SQL (though this is far from reality). 
I am also well aware that databases are not the only place that data can be extracted. Some other data sources include:

1. Web scraping
2. Flat files (excel, csv, json)
3. Logs

I am familiar wth all the sources named in this list with the exception of logs. Have I looked at or seen logs? Of course
I have. I still refer to my **dmesg** log if I have a file system or device mounting issue. I have used numerous
apache/httpd logs to troubleshoot **WSGI** applications. But, both of these cases have never merited me learning to use something
like a log aggregator. Until now. For the first time in my professional career, I have found the need to extract log data
which is not available anywhere else except in a mountain of logs. More specifically, logs generated by automation systems.
Since I have read about various solutions for successfully analyzing log data in the past, I have decided to use this
opportunity to test drive some of these solutions. I will not be using the automation logs already mentioned above
for this post, but I will be using various sytems logs to better understand the tooling and log aggregate capabilities
in general.

## Grafana Loki

Loki is a log aggregation system, inspired by Prometheus, which was started in 2018 by Grafana labs. I will be using
the local install (non-Docker on bare metal), version 2.7.1. I am not going to run through the install process. In short, the
Loki archive came with a single binary included. A configuration file is required to indicate the desired location for stored
data, the listening port number, etc.. The installation process is relatively straightforward and the only issues encountered
were due to improper permissions. I have included my configuration file which I am using for my test drive below. 

### Loki configuration file used

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /tmp/loki
  storage:
    filesystem:
      chunks_directory: /tmp/loki/chunks
      rules_directory: /tmp/loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

limits_config:
  max_query_length: 0h

query_scheduler:
  max_outstanding_requests_per_tenant: 4096

frontend:
  max_outstanding_per_tenant: 4096

ruler:
  alertmanager_url: http://localhost:9093

analytics:
  reporting_enabled: false
```

I have also created a service file to start the Loki daemon quickly without the need to remember how to source the
configuration file. I am including this as a reference for myself and any reader who stumbles upon this post who uses systemd.

```conf
[Unit]
Description=Loki service
After=network.target

[Service]
Type=simple
User=loki
ExecStart=/usr/local/bin/loki-linux-amd64 -config.file /usr/local/bin/loki-local-config.yaml

[Install]
WantedBy=multi-user.target
```

So, now that I have Loki downloaded, configured and running, I need to start sending some log streams to the Loki api. To
accomplish this task I will be using Promtail. The next section will briefly cover Promtail and also include the configuration
and service file.

## Promtail

Promtail should be installed on any system containing logs. Promtail runs as a background service and will monitor the log 
files and extract any newly appended log entries from those log files. Once extracted the log entries will be labled and pushed 
to the Loki server which is actively listening (at port 3100 according to the configuration file above).

### Promtail configuration file used

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
- job_name: system
  static_configs:
  - targets:
      - localhost
    labels:
      job: varlogs
      __path__: /var/log/*log
- job_name: apache
  static_configs:
  - targets:
     - localhost
    labels:
      job: apache
      __path__: /var/log/apache2/access.log
```

...and the service file used to start the Promtail service daemon on my systemd system:

```conf
[Unit]
Description=Promtail service
After=network.target

[Service]
Type=simple
User=promtail
ExecStart=/usr/local/bin/promtail-linux-amd64 -config.file /usr/local/bin/promtail-local-config.yaml

[Install]
WantedBy=multi-user.target
```

I would like to mention now that the Promtail configuration file can be used to create labels (think of a label as something
you might want to group by during aggregation) before the log stream is sent to the Loki listener. Reading some of the 
documentation initially, I was under the impression that this was how Loki **should** be used. As such, the simple configuration
file shown above was loaded with pipelines containing some serious regular expressions to pre-process the logs. After much
frustration and after reading a multitude of Grafana community posts, I have come to the realisation that **creating Labels is
best handled at query time**. Too many labels leads to issues concerning series cardinality. It is also painful to test regex by
continuously stopping and restarting the Promtail daemon (I am not a regex pro in all the flavors of regex that are used today, 
Loki and Promtail understand _Go RE2 regex strings_). I will discuss these points more later in the post.

## Connecting to Grafana

If you are familiar with Grafana, then you already know that the next step is to create a new data source, so I will do just
that. For testing and for the purpose of this post, I am running Loki, Promtail and Grafana, all on the same computer. With this
in mind, here is my new data source:

<img src="/assets/loki_data_source.png" alt="drawing" style="max-width: 100%; height: auto; text-align: center;"/>

Next I will:

1. Create a new dashboard
2. Select the newly created Loki source as my data source
3. Choose job under label filters
4. Select apache which is the job name I gave the apache access.log in the Promtail config
5. Generate a table to inspect how Loki is representing the log data

### Grafana table panel output: apache2 access.log

<img src="/assets/loki_apache_table.png" alt="drawing" style="max-width: 100%; height: auto; text-align: center;"/>

### Grafana log panel output: apache2 access.log

<img src="/assets/loki_grafana_log_panel.png" alt="drawing" style="max-width: 100%; height: auto; text-align: center;"/>

(POST CURRENTLY UNDER CONSTRUCTION AND NOT COMPLETE)
