<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-12-21T21:12:38+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">A life that made sense</title><subtitle>Digital musings</subtitle><author><name>Mike Hinkle</name></author><entry><title type="html">Has the world lost its momentum?</title><link href="http://localhost:4000/remote/work,/teletravail/2022/12/05/remote_work.html" rel="alternate" type="text/html" title="Has the world lost its momentum?" /><published>2022-12-05T09:00:00+01:00</published><updated>2022-12-05T09:00:00+01:00</updated><id>http://localhost:4000/remote/work,/teletravail/2022/12/05/remote_work</id><content type="html" xml:base="http://localhost:4000/remote/work,/teletravail/2022/12/05/remote_work.html">&lt;h3 id=&quot;a-quick-message-to-the-reader&quot;&gt;A quick message to the reader&lt;/h3&gt;

&lt;p&gt;This post is the product of working the past two long years, at two large companies, in 
two different countries and in two different sectors. If you expect to learn something
profound reading this, I can almost assure you that you will not. My motivation is 
only to put my frustrations to writing and to state my case against remote work. I am of
the opinion that years from now, when scholars are studying the collapse of civilization
as it is today, they will most certainly attribute our fall, at least in part, to social 
media and remote work. Today I will focus on the latter of the two topics as it is a subject
near and dear to my heart.&lt;/p&gt;

&lt;h3 id=&quot;the-context&quot;&gt;The context&lt;/h3&gt;

&lt;p&gt;I thought COVID-19 would be over within a few weeks. God was I wrong. Despite
the fact that I am not wearing a mask this week (I did have to wear one in a meeting
only last week), there has been a fundamental shift to the way we live life and conduct
business in this semi-post COVID world. One of those fundamental shifts has been the 
normalization of remote work (known as télétravail in my new home country). I know, it
sounds like a dream come true. On &lt;em&gt;Hacker News&lt;/em&gt; I frequently read individuals touting
their great productivity while working remotely. They were never as productive before
as they are now, sitting or lounging wherever they are, away from their colleagues and
their stuffy offices while checking their morning emails. Perhaps this is entirely true,
I really do not know. I can only speak to how it has felt working onsite for the past
few years while many of my colleagues opted to work remotely. Maybe my experiences, being
on the receiving end of remote work are unique, but I doubt this is the case.&lt;/p&gt;

&lt;h3 id=&quot;in-the-beginning&quot;&gt;In the beginning&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dylan_birth2.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the stay-at-home orders were initially issued, my employer was deemed an essential
entity and therefore continued to operate 24 hours a day, 7 days a week. My son was about 
to be born and I was fortunate to be provided with paternity leave as a father. 
Of course I took every bit of the time off and then some. Most of my time off was to 
help my wife with the kids (especially my son who seemed to not sleep) but I also did 
not want to go to work. Little was known about COVID at the time, and I would check daily, 
with impending doom, how many more little red dots had appeared near me on the Johns Hopkins 
COVID dashboard. In the end, my paid time off was running low and I decided that it was high time to 
return to work. I was an engineer but my position also placed me in direct supervision of approximately 
15 people who were required to be onsite. Due to my position, HR gave me the option to
work remotely. While the offer was tempting, I do not believe I would have, nor should have been
respected by my direct reports. If they had to be onsite, I should be onsite. And so I went…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/jh_dashboard.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;insult-to-injury&quot;&gt;Insult to injury&lt;/h3&gt;

&lt;p&gt;I had two offices on different floors. Prior to the pandemic, I would frequently occupy
the office which would allow me to focus on my work with the fewest amount of interruptions.
I promise that I was not shirking my responsibilities, but aside from my day-to-day
duties I also worked on many projects which required some quiet and my undivided attention.
Now, with the pandemic in full swing, I did not have to play hide-and-seek to focus
on my work, everything was quiet… in fact too quiet.&lt;/p&gt;

&lt;p&gt;Since I had been out for over a month (I &lt;em&gt;had&lt;/em&gt; been in frequent contact with my team by phone
and via email), I spent my first day or two checking in on my team. How were they doing?
How were their families? How has work been? This final question illicited the most responses
and complaints. It seems that practically no one from management had been onsite since the stay 
at home order was issued (about one month). The only form of communication was coming in the
form of an email sent every few days from HR and upper management to the building. These emails
would announce the latest butcher’s bill of COVID positive individuals in the building. 
Accompanying these numbers was always a message to remind everyone that their safety was the
companies number one priority. This message seemed to be the root of most ill feelings
amongst those the people onsite (myself included).&lt;br /&gt;
Given the backdrop of everything happening, the messaging did feel disengenuous. To this day I
shudder thinking about those emails. If this was really the case, everyone would be home, right?
But, I digress. This post is not about distasteful emails but remote work and apparently most, if not 
all of management, including engineering was doing just that.. working remotely.&lt;/p&gt;

&lt;h3 id=&quot;radio-silence&quot;&gt;Radio silence&lt;/h3&gt;

&lt;p&gt;Initially, the quiet was a welcome reprieve from the typical chaos which permeated our factory.
I had a number of ideas in my time away from work that I wanted to bring to life and the
silence allowed me to do just that. I quickly noticed that when I did need to send an email to
a colleague, that responses were not as punctual as they seemed to have been before the stay-at-home.
Maybe it was just me? At the time, I asked myself this question (I am not the most patient 
individual in the world), but today I am convinced otherwise. The more and more I needed an answer 
from planning, product engineering, from other engineers, the more I came to the realization that people 
could not really be on their computers, logged in doing real &lt;em&gt;work&lt;/em&gt;, all day. They just couldn’t. 
Many of my requests were critical to customers receiving their semiconductors on time. As an example,
our planning department had the capability of flagging wafer lots as a priority. A wafer lot with a priority
flag of zero was the highest priority, then priority one and finally priority two. Typically, priority
wafer lots were closely monitored and if placed on hold for more than 30 minutes, phones would begin to
ring asking why the lot was not being processed. We had rules on the books about this very topic. I was
at the very end of our factory flow in the probe and test area, so I spoke frequently with planning and
would field their phone calls almost every morning. These phone calls stopped coming in almost completely.
One morning I returned after a long weekend to find a priority zero lot had been on hold for longer than 
72 hours and no one seemed to care. No emails, no phone calls, nothing… The 30 minute rule which was
sacrosanct was being violated and nothing… Radio silence.&lt;/p&gt;

&lt;p&gt;I could share a laundry list of examples similar to this, but I will not. I assure you that there
were many many more. Needless to say that if I could feel the lack of support from all of leadership
in my company, so could my direct reports. After a few months of this radio silence, my wife and I
decided to take the children and relocate to France. My wife is French, so this was not a random choice.
Other factors in our decision were, my wife has a large family in France (our daughter is severely handicapped
so the family support is a great help), the area of France we were moving to (now live in) is much safer
than Dallas, Texas (where we were living) for my children to grow up, and finally, I realized that my work was just that, work.
I was payed very well, I was given the tools to create great things and was left alone for the most part
to be autonomous, but it really didn’t feel like anyone cared anymore. So, after some introspection I decided
that money isn’t everything and why not take the plunge and move. I will find another job where people
are engaged and passionate about what they do. So we sold the house, shipped our belongings across
the Atlantic and moved.&lt;/p&gt;

&lt;h3 id=&quot;no-more-momentum&quot;&gt;No more momentum&lt;/h3&gt;

&lt;p&gt;Fast forward a year and a half. I have found a job doing technical things in a manufacturing environment
which I enjoy (always different problems and so much to be improved). I have a great team who has looked past
my rudimentary but steadily improving French (I admit my improvement is slower than I would like).
By all accounts, everything is better than I could have hoped. So why am I writing this now? I am writing
this because the lack of engagement, enthusiasm and passion which I felt before leaving my last place of
employee, seems to have followed me across the Atlantic and contaminated Europe. Email replies or messaging frequently draw slow responses.
My manager requested access for me to a specific system from someone whose primary duty is to grant that
access and approximately one month came and went before I could access the system. It is so ridiculous
that all I can do is laugh. I feel like the world was hobbling slowly forward before the pandemic and the worldwide
closures completely stopped what little momentum we had. I deal with many suppliers in other countries
and I experience this lack of engagement everywhere I look with almost every company I deal with. These
are not one off observations, this seems to be how the world works now.&lt;/p&gt;

&lt;h3 id=&quot;first-step-towards-a-solution&quot;&gt;First step towards a solution&lt;/h3&gt;

&lt;p&gt;After much thought on the subject, I have become convinced that the first tangible step employers can take
to restore that momentum is by forcing their employees back onsite. I know this is an unpopular belief, but can
we all honestly say that our business’ productivity right now is better than it was before the pandemic?
Perhaps some institutions or companies are, but by and large I would argue that they are not. Even as a consumer it has been
a struggle dealing with everyone from our old gas company, to the architect we hired to make a simple drawing (six months and
the architect still has not finished our drawing). Accomplishing the simplest tasks requiring input from someone could take weeks. 
Is the individual on vacation? Does the person have COVID? Maybe they work in another group now? I really don’t know.
If we could only begin communicating with each other again, we could go back to solving good problems. And for me, walking
to my colleagues office to quickly get a question answered seems so much more efficient than sending an email and waiting.
I have grown tired of waiting.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“No man is an island entire of itself; every man 
is a piece of the continent, a part of the main; 
if a clod be washed away by the sea, Europe 
is the less, as well as if a promontory were, as 
well as any manner of thy friends or of thine 
own were; any man’s death diminishes me, 
because I am involved in mankind. 
And therefore never send to know for whom 
the bell tolls; it tolls for thee.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;-John Donne: No Man is an Island&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="remote" /><category term="work," /><category term="teletravail" /><summary type="html">A quick message to the reader</summary></entry><entry><title type="html">Apache Superset: Test driving a new tool with familiar metrics</title><link href="http://localhost:4000/superset/postgresql/2021/08/02/superset.html" rel="alternate" type="text/html" title="Apache Superset: Test driving a new tool with familiar metrics" /><published>2021-08-02T17:00:00+02:00</published><updated>2021-08-02T17:00:00+02:00</updated><id>http://localhost:4000/superset/postgresql/2021/08/02/superset</id><content type="html" xml:base="http://localhost:4000/superset/postgresql/2021/08/02/superset.html">&lt;p&gt;For years, I have been using a piece of software by the name of &lt;em&gt;Spotfire&lt;/em&gt;, Tibco Spotfire to be more specific. The software
falls into the category of what is referred to these days as &lt;strong&gt;BI&lt;/strong&gt; or &lt;em&gt;Business Intelligence&lt;/em&gt; software :chart_with_upwards_trend:. While I was never
a huge proponent of Spotfire because there were limitations on what it could do, I was still pretty decent at embedding R &amp;amp; SQL,
creating templates and ultimately deploying the dxp files to the web (via WebPlayer). Spotfire is proprietary and licensing can cost big money, so 
I used it at work and work alone.&lt;/p&gt;

&lt;p&gt;:books: I trained many engineers at my previous employer in SQL, R, Python, Linux, etc.. But, the request I received most
often, was can I train them on getting the most out of Spotfire. In fact my final few months, I was giving Spotfire trainings
to various groups multiple times a week. Given that these business intelligence tools are becoming common place at most
companies and that they allow people who do not necessarily nerd out on application and web developement, the capability
to create their own reports or data-driven applications with relative ease… Why not spend some time to explore a relatively 
new Open Source BI tool by the name of Apache Superset?&lt;/p&gt;

&lt;h2 id=&quot;superset-install-and-test-drive&quot;&gt;Superset: Install and test drive&lt;/h2&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;p&gt;According to the Apache Superset documentation found &lt;a href=&quot;https://superset.apache.org/&quot;&gt;here&lt;/a&gt;, there are two ways to install
Superset (at least for Linux):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using docker-compose&lt;/li&gt;
  &lt;li&gt;Using pip&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I began by testing the software using the &lt;strong&gt;docker-compose&lt;/strong&gt; install. The install was very straightforward and I was able
to login using admin/admin as my username and password. After logging in I was able to connect to a local instance of PostgreSQL
without any issue. Before getting too carried away, I decided to logout and stop and remove the containers before firing the
containers immediately back up. I am happy I did beause I was greeted with an error and could not for the life of me, restart
the containers. So, onto using pip in a virtual environment (option 2 above).&lt;/p&gt;

&lt;p&gt;I used the following steps to install Superset with pip:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Installed OS dependencies: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install build-essential libssl-dev libffi-dev python3-dev python3-pip libsasl2-dev libldap2-dev&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Created an empty directory for my virtual environment and superset: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir superset&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Changed directories into that empty directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd superset&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Created the python virtual environment: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python3 -m venv venv&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Activated the virtual environment: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;. venv/bin/activate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Installed Apache Superset: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install apache-superset&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Created username and set password: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;superset fab create-admin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Created roles and permissions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;superset init&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Installed psycopg2 since I am connecting to a PostgreSQL database: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install psycopg2&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Finally, started the flask development web server: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;superset run -p 8088 --with-threads --reload --debugger&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After performing the above steps, I was able to open my web browser on my localhost at port 8088 and login to
Apache Superset. Connecting to my PostgreSQL database server was very easy. I selected &lt;em&gt;Data&lt;/em&gt; on the tap nav bar and
&lt;em&gt;Databases&lt;/em&gt; from the dropdown. Once on the database page, I selected the &lt;em&gt;+ DATABASE&lt;/em&gt; button in the upper right hand
side of the page opening this menu :point_down::&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/superset_add_db.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The display name can be anything you really want, I would suggest something that describes the type of data stored
in the database you are connecting to. My connection string for my PostgreSQL database looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgresql+psycopg2://soitgoes511:XXXXXXXXXX@192.168.0.12:5432/pulse_oximeter_historic
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you believe you will ever upload CSV files for analysis, this is where you configure that. Go to &lt;em&gt;ADVANCED&lt;/em&gt; -&amp;gt; &lt;em&gt;Other&lt;/em&gt; -&amp;gt;
toggle &lt;em&gt;Allow data upload&lt;/em&gt;. If you have selected this option and upload a CSV file, that CSV file will be loaded as a
table into the database you just created the connection for.&lt;/p&gt;

&lt;p&gt;After completing the above steps, I quickly faced an issue. Despite being able to connect to my database succesfully,
when attempting a simple query, I was greeted with the below error:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/psycopg2_error.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;
  &lt;figcaption&gt;Error appearing on each query attempt in superset if dataset contained timestamp with tz offset.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There is still an open &lt;a href=&quot;https://github.com/apache/superset/issues/15768&quot;&gt;issue&lt;/a&gt; on Github concerning the error above.
Long story short, the error is appearing due to a timestamped column and more specifically, a timestamped column
containing an offset. To head off any headaches, if you see this error, check your &lt;strong&gt;psycopg2&lt;/strong&gt; version. Myself as
well as at least one other individual had version 2.9.1 installed and downgrading to &lt;strong&gt;psycopg2==2.8.6&lt;/strong&gt; made the error
go away. :heavy_check_mark:&lt;/p&gt;

&lt;p&gt;Now that Apache Superset is installed, my database is connected and the error is gone, I can move forward with creating datasets,
charts and dashboards.&lt;/p&gt;

&lt;h4 id=&quot;creating-a-dataset&quot;&gt;Creating a dataset&lt;/h4&gt;
&lt;p&gt;The entire spirit of business intelligence tools is to explore data, look for insights and hopefully solve some good problems. Once
you have the story you would like to tell laid out on a dashboard, you can share this with your colleagues. On the job, I have
made it a habit of trying to condense all of my biggest care abouts down to two or three views or dashboards. If there were certain
data points co-workers or managers would ask for often, I made sure that was on one of my dashboards. This allowed me to identify
issues, near real time, and be reactive. This also freed up my time to dig into new interesting problems. Some people like the mindless
task of spending hours, manually compiling their data and reports, I prefer to automate it.&lt;/p&gt;

&lt;p&gt;So, to create these dashboards, we need data. Superset will allow you to import CSV files or query the database we have already
connected to. Since I enjoy SQL, and have some data I would like to review stored in my PostgreSQL db, I will walk through 
querying my a PostgreSQL database in Superset via &lt;strong&gt;SQL Lab&lt;/strong&gt; and then saving that dataset and query.&lt;/p&gt;

&lt;p&gt;The first step is to select &lt;em&gt;SQL Lab&lt;/em&gt; on the top nav bar, and from the dropdown, select &lt;em&gt;SQL Editor&lt;/em&gt;. You will see this, but
without the query and return values:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/superset_sql_lab.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please note, that you select the database connection you would like to use in the upper left hand corner (pulse_ox in my case).
Once I entered the query seen below, I can hit &lt;strong&gt;ctrl+enter&lt;/strong&gt; to sanity check and review the data. Here is a closer look at my
query shown in the screenshot:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; 
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;TIME&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ZONE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;CEST&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ma_spo2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ma_bpm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ma_perf&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;pulse_ox_moving_average&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; 
  &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NOW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INTERVAL&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;1 hour&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, before I save this query and the dataset, I will remove the &lt;strong&gt;WHERE&lt;/strong&gt; and &lt;strong&gt;ORDER BY&lt;/strong&gt; clauses. I do not want to filter
the data in the dataset or underlying query but with a filter on the dashboard. I also do not want to order the dataset since
all it will do is waste compute time and is unnecessary. Also note that I am converting my time to &lt;em&gt;Central European Summer Time&lt;/em&gt;.
I have the timestamps stored as &lt;em&gt;UTC&lt;/em&gt; in my postgres instance and would like to review the traces in my local timezone.&lt;/p&gt;

&lt;p&gt;To save the query for later use or fine-tuning, select the blue &lt;strong&gt;SAVE&lt;/strong&gt; button under the text box. To save the dataset and begin
charting, select the blue &lt;strong&gt;EXPLORE&lt;/strong&gt; button directly above the return results. Once named and saved, a new &lt;strong&gt;EXPLORE&lt;/strong&gt; tab will
be opened in the brower, and I can begin building my charts.&lt;/p&gt;

&lt;h4 id=&quot;creating-charts-and-filters&quot;&gt;Creating charts and filters&lt;/h4&gt;

&lt;p&gt;Creating charts is fairly straight forward. There are a multitude of chart types to choose from:
&lt;img src=&quot;/assets/superset_chart_types.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I have not experimented with them all but I have with most. Since my data is time-series data, I will choose the time-series
chart type (last chart in the above assortment). This chart type does not have the ability to use a left and a right y-axis,
but I can create multiple traces on the same plot which share the same y-axis. Here is my first attempt of a chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/superset_explore_chart.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When creating a dashboard in Superset, you choose from your already created and available charts and/or filters. So, I will
go ahead and create a simple time range filter and some more charts before I move forward with my dashboard. Here is a simple
filter (found as a chart type):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/superset_filter.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am even able to build forecasting into a chart using &lt;strong&gt;Prophet&lt;/strong&gt;. The component plots are not available, but designating
my confidence interval, daily, weekly and yearly seasonality is available. To utilize this feature, you must install
pystan and prophet:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;pip –no-cache-dir install pystan==2.19.1.1&lt;/li&gt;
  &lt;li&gt;pip install prophet&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As of this post, pystan &amp;gt;= version 3.0 will not work, hence me specifying version 2.19.1.1 above.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/superset_bpm_prophet.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;
  &lt;figcaption&gt;Prophet forecast of my daughters heart rate.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;putting-it-all-together-into-a-dashboard&quot;&gt;Putting it all together into a dashboard&lt;/h4&gt;

&lt;p&gt;Once all of the charts and filters have been created, it is time to construct a dashboard. Don’t fret if something needs
tweaked further down the road, all you have to do is update the chart and the dashboard (assuming it contains the chart)
will reflect the updates.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/assets/superset_pox_dash_pic1.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;
  &lt;figcaption&gt;Moving average pulse oximetry metrics materialized with dbt and queried from postgres.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/prophet-models-health-superset.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;
  &lt;figcaption&gt;3-day heart rate and temperature forecast with 90% confidence interval. Forecast generated using
    Prophet and configured easily in Apache Superset.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;closing-comments&quot;&gt;Closing comments&lt;/h4&gt;

&lt;p&gt;I mentioned Spotfire at the beginning of this post, so, after walking through creating datasets, charts and dashboards
with Apache Superset I should mention a few glaring differences between the two BI tools.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spotfire is proprietary and Superset is open source&lt;/li&gt;
  &lt;li&gt;I commonly used R to wrangle and transform data in Spotfire as a &lt;strong&gt;Data Function&lt;/strong&gt;,
 Superset allowed for only SQL (easier but limiting)&lt;/li&gt;
  &lt;li&gt;Spotfire could run as a free standing desktop application on my PC or be deployed
 to the web and accessed via Webplayer. Superset runs as a standalone server on some system.
 I have seen individuals deploying Superset to Heroku.&lt;/li&gt;
  &lt;li&gt;There were more chart types available for Superset but some of the charts I have had
 difficulty using (&lt;strong&gt;Multiple Line Charts&lt;/strong&gt; specifically). Superset is still relatively new
 and I have faith that some of the bugs will be worked out soon.&lt;/li&gt;
  &lt;li&gt;Errors are easier to interpret (IMHO) in Superset than Spotfire. I don’t know how
 many times I had some hidden whitespace character in my Spotfire Data Function that
 was preventing my function from executing. The errors would yield zero valuable
 insight.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many facets of Apache Superset I did not delve into (users, roles, annotations and layers,
deploying on a production server, etc…). The primary motivation for this post was to show
that good business intelligence tools are available to use for free, and can help you gain
insight into whatever data you are attempting to put under a microscope. Why not take advantage
of them if the need arises?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“There was nowhere to go but everywhere, so just keep on rolling under the stars.”
― Jack Kerouac, On the Road: the Original Scroll&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="superset" /><category term="postgresql" /><summary type="html">For years, I have been using a piece of software by the name of Spotfire, Tibco Spotfire to be more specific. The software falls into the category of what is referred to these days as BI or Business Intelligence software :chart_with_upwards_trend:. While I was never a huge proponent of Spotfire because there were limitations on what it could do, I was still pretty decent at embedding R &amp;amp; SQL, creating templates and ultimately deploying the dxp files to the web (via WebPlayer). Spotfire is proprietary and licensing can cost big money, so I used it at work and work alone.</summary></entry><entry><title type="html">A simple use case for dbt and Airflow</title><link href="http://localhost:4000/rpi/postgres/dbt/airflow/2021/07/20/dbt_install_transform.html" rel="alternate" type="text/html" title="A simple use case for dbt and Airflow" /><published>2021-07-20T17:00:00+02:00</published><updated>2021-07-20T17:00:00+02:00</updated><id>http://localhost:4000/rpi/postgres/dbt/airflow/2021/07/20/dbt_install_transform</id><content type="html" xml:base="http://localhost:4000/rpi/postgres/dbt/airflow/2021/07/20/dbt_install_transform.html">&lt;p&gt;My previous two posts involved installing a postgres server on a cheap, spare raspberry pi 3b+.
The motivation was to save my time-series data for longer than 30-days since my free InfluxDB Cloud
account, only has a 30-day retention policy.&lt;/p&gt;

&lt;p&gt;I was successful in installing a fresh OS, configuring the &lt;strong&gt;rpi&lt;/strong&gt; to run headless, scanning the local
ip’s to find the rpi, ssh’ing to the single board computer, installing a &lt;strong&gt;PostgreSQL&lt;/strong&gt; instance and
finally writing a script to query my influxDB instance, transform the data and push the data to my
postgresql database. I used a cronjob to execute the script with a comment about accomplishing
the same task using &lt;strong&gt;Apache Airflow&lt;/strong&gt;. So today, I am back to offload the scheduling from the cron daemon
to Airflow and I am also throwing a &lt;strong&gt;dbt&lt;/strong&gt; incremental materialization into the mix.&lt;/p&gt;

&lt;h3 id=&quot;motivation-for-added-complexity&quot;&gt;Motivation for added complexity&lt;/h3&gt;

&lt;p&gt;You might say that the cronjob was working just fine and if it isn’t broken, why fix it :wrench: ? I
even commented during the writing that using Airflow felt like bird hunting with a scud missile. 
Despite my comment, I have a simple and good reason to make use of Airflow.&lt;/p&gt;

&lt;p&gt;The pulse oximeter data 
I am collecting is very noisy. More so when my daughter moves, the sensor is not attached well or 
the sensor could even be defective. Whatever the case may be, outliers are not uncommon. Here is
a screenshot to illustrate my point. The orange trace below represents the mean over a five second
rolling window of my daughter’s heartrate. So, these data have already been “smoothed” slightly
considering I capture her &lt;strong&gt;heartrate&lt;/strong&gt;, &lt;strong&gt;specific oxygen&lt;/strong&gt; and &lt;strong&gt;perfusion index&lt;/strong&gt; all at a frequency of one 
sample per second.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bpm_five_sec_agg.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would like to smooth the signal even more using something simple like a moving average. This
would allow me to more easily spot trends. Since I look at my daughter’s trends regularly, I need
the data to be relatively up-to-date. I also take snapshots from my cloud database on an
hourly basis, so it would be nice to calculate a moving average for my metrics of interest at the
same time and frequency the updates occur. In this spirit, I decided to use dbt (&lt;strong&gt;D&lt;/strong&gt;ata &lt;strong&gt;B&lt;/strong&gt;uild &lt;strong&gt;T&lt;/strong&gt;ool) to perform the
transformation and materialization, and Airflow to ensure that this dbt model runs after and only
after the data is loaded into postgresql from my cloud instance. So, let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;dbt-install-init-model-creation-and-test&quot;&gt;dbt: Install, init, model creation and test&lt;/h2&gt;

&lt;p&gt;I will not delve into all of the fine details of dbt, the &lt;a href=&quot;https://docs.getdbt.com/docs/introduction&quot;&gt;documentation&lt;/a&gt;
found online is excellent and I encourage you if you are interested in databases to check it out yourself.
I will say that dbt will allow me to easily perform a transformation on data in my postgresql database using
a simple query. After the transformation has been performed, dbt will handle the materialization for me.&lt;/p&gt;

&lt;h3 id=&quot;dbt-install-and-initialization&quot;&gt;dbt: Install and initialization&lt;/h3&gt;

&lt;p&gt;Dbt is a python module and can therefore be installed with pip. Using &lt;em&gt;pip&lt;/em&gt; I installed dbt-postgres
on my rpi3b+: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python3 -m pip install dbt-postgres&lt;/code&gt;. I specified PostgreSQL because I am
transforming data on a postgres server and dbt will need the postgres adapter to interact with the
database. Some other supported databases include: BigQuery, Redshift and Snowflake.&lt;/p&gt;

&lt;p&gt;After the install was complete, I initialized a project called &lt;strong&gt;health_metrics&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dbt init health_metrics --adapter postgres
Running with dbt=0.20.0
Creating dbt configuration folder at /home/pi/.dbt
With sample profiles.yml for postgres

Your new dbt project &quot;health_metrics&quot; was created! If this is your first time
using dbt, you&apos;ll need to set up your profiles.yml file (we&apos;ve created a sample
file for you to connect to postgres) -- this file will tell dbt how
to connect to your database. You can find this file by running:

  xdg-open /home/pi/.dbt

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don&apos;t hesitate to reach out to us via GitHub issues or on Slack --
There&apos;s a link to our Slack group in the GitHub Readme. Happy modeling!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, I need to update the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;profiles.yml&lt;/code&gt; file as indicated by the message above
seen after initilizing my dbt project. Below shows my updated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;profiles.yml&lt;/code&gt; file
with the database password excluded:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  
    &lt;span class=&quot;na&quot;&gt;dev&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;postgres&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;threads&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.12&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5432&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;soitgoes511&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;pass&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;PASSW_OF_DB&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;dbname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pulse_oximeter_historic&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;public&lt;/span&gt;
      
    &lt;span class=&quot;na&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;postgres&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;threads&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.12&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5432&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;soitgoes511&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;pass&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;PASSW_OF_DB&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;dbname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pulse_oximeter_historic&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;public&lt;/span&gt;
    
  &lt;span class=&quot;na&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dev&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once my &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;profiles.yml&lt;/code&gt; configuration has been completed, I can save the file
and move on to writing the actual model.&lt;/p&gt;

&lt;h3 id=&quot;dbt-model-creation-and-test&quot;&gt;dbt: Model creation and test&lt;/h3&gt;

&lt;p&gt;A model in it’s most basic form is an &lt;strong&gt;.sql&lt;/strong&gt; file containing a single SQL 
&lt;strong&gt;SELECT&lt;/strong&gt; statement. Included in the same directory as the &lt;strong&gt;.sql&lt;/strong&gt; file is a
file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schema.yml&lt;/code&gt; containing the model name, the column names being 
returned by the &lt;strong&gt;SELECT&lt;/strong&gt; statement, a brief description of each column and
test assertions which can be used to validate your model is performing the way
you expect it to. Further down the road, your model can be tested simply
by executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dbt test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So, to the task at hand. I wanted to remove some noise and smooth my data
and will start by calculating a simple moving average. My original data
has been aggregated using the mean over a five second window before it is loaded 
into my postgresql database. This means I should have 12 samples per minute.
I have been looking at pulse oximeter data for years now and feel confident
that a five minute window will remove most of the noise without losing too
much information. I can always adjust later, or revert back to my 5 second
aggregated data. No harm, no foul. So, 12 samples per minute for 5 minutes
equates to 60 data points (I need this for my query below).&lt;/p&gt;

&lt;p&gt;One more point to cover before getting to the actual query/model. Dbt supports
templating, macros, references, etc.. The possible materializations include
a table, view, incremental and ephemeral. &lt;strong&gt;Table&lt;/strong&gt; and &lt;strong&gt;View&lt;/strong&gt; are hopefully
self-explanatory. If not, please refer to the documentation link I referenced
earlier in this writing. I will be using &lt;strong&gt;incremental&lt;/strong&gt;. Incremental for the
first execution will build a complete table. For later model runs,
dbt will only build the new data onto the table assuming I have the &lt;strong&gt;is_incremental()&lt;/strong&gt;
macro wrapping my filters used to specify the new data. Again, the dbt folks do a much better 
job explaining this and that explanation can be found 
&lt;a href=&quot;https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, here is the first iteration of my moving average model which will build
a table, incrementally (completely at first and can be rebuilt anew if
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--full-refresh&lt;/code&gt; flag is used at runtime):&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;materialized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;incremental&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;spo2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spo2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PRECEDING&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROW&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ma_spo2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;bpm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bpm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PRECEDING&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROW&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ma_bpm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;perf_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perf_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PRECEDING&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ROW&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ma_perf&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pox_five_second_mean&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_incremental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The contents of my &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schema.yml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pulse_ox_moving_average&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;minute&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;moving&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;spo2,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;bpm&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;perf&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;time&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;The&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;primary&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;table&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spo2&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SPO2&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sensor&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;aggregated&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;minutes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;series&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ma_spo2&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Minute&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;moving&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SPO2,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;previous&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;plus&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;current&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bpm&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;BPM&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sensor&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;aggregated&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;minutes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;series&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ma_bpm&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Minute&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;moving&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;BPM,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;previous&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;plus&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;current&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;perf_index&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Perfusion&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Index&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sensor&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;aggregated&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;minutes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;series&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ma_perf&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Minute&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;moving&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Perfusion&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Index,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;previous&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;plus&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;current&quot;&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;not_null&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally, let me run the model and see what happens:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dbt run
Running with dbt=0.20.0
Found 1 model, 7 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures

22:45:55 | Concurrency: 1 threads (target=&apos;dev&apos;)
22:45:55 |
22:45:55 | 1 of 1 START incremental model public.pulse_ox_moving_average........ [RUN]
22:45:56 | 1 of 1 OK created incremental model public.pulse_ox_moving_average... [INSERT 0 0 in 0.84s]
22:45:56 |
22:45:56 | Finished running 1 incremental model in 1.41s.

Completed successfully

Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice the relatively fast time of execution. That is because I have run this model before and there is no new
data loaded to compute the moving average. The actual runtime is about 22 seconds, please bare in mind this is an
oldish rpi3b+ and not a supercomputer. Also note, that if I had specified a &lt;strong&gt;table&lt;/strong&gt; for my materialization,
the entire table would have been rebuilt and this would have taken even longer than 22 seconds. If I were
using a pay-per-use cloud provider, that would most likely equate to money down the drain :money_with_wings: . Here is a sample of the
result of my successful model materialization:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ma_post_test.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that the model is working and has been tested, it is time to automate the model run at the top of each hour.
Let’s move onto setting up Apache Airflow and creating a DAG (&lt;strong&gt;D&lt;/strong&gt;irected &lt;strong&gt;A&lt;/strong&gt;cyclic &lt;strong&gt;G&lt;/strong&gt;raph).&lt;/p&gt;

&lt;h2 id=&quot;apache-airflow-install-setup-and-dag-creation&quot;&gt;Apache Airflow: Install, setup and DAG creation&lt;/h2&gt;

&lt;p&gt;Airflow was created by Maxime Beauchemin and used at Airbnb in 2014 to programmatically schedule tasks. Since
that time, Airflow has become a part of the Apache Software Foundation.&lt;/p&gt;

&lt;p&gt;I decided to change from my simplistic cron schedular to Airflow to ensure that the moving average is not
calculated until after the pulse oximeter data has been retrieved from my InfluxDB instance, shaped and
loaded into the PostgreSQL database server running on my raspberry pi. I could have handled this with
a cronjob in quite a few ways (e.g. Serializing the data extraction and loading with the moving average
calculation and table creation in the same script or simply by skewing the timing of two different
crons which is an error prone hack imho). While this is a simplistic use case, it still
solves my script execution timing issue in an elegant manner (with some added complexity). :grin:&lt;/p&gt;

&lt;h3 id=&quot;airflow-install-and-setup&quot;&gt;Airflow: Install and setup&lt;/h3&gt;

&lt;p&gt;I used the following steps to install Airflow on my raspberry pi. Please be aware that these steps were
taken directly from the Apache Airflow docs which can be found &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/start/local.html&quot;&gt;here&lt;/a&gt;.
I have not altered nor improved these steps in any way and in no way am taking any credit for this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# airflow needs a home, ~/airflow is the default,&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# but you can lay foundation somewhere else if you prefer&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (optional)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/airflow

&lt;span class=&quot;nv&quot;&gt;AIRFLOW_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2.1.2
&lt;span class=&quot;nv&quot;&gt;PYTHON_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;python &lt;span class=&quot;nt&quot;&gt;--version&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; 2 | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;.&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; 1-2&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For example: 3.6&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CONSTRAINT_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/apache/airflow/constraints-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/constraints-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PYTHON_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.txt&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-3.6.txt&lt;/span&gt;
pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;apache-airflow==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--constraint&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CONSTRAINT_URL&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# initialize the database&lt;/span&gt;
airflow db init

airflow &lt;span class=&quot;nb&quot;&gt;users &lt;/span&gt;create &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--firstname&lt;/span&gt; Peter &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--lastname&lt;/span&gt; Parker &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--role&lt;/span&gt; Admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--email&lt;/span&gt; spiderman@superhero.org

&lt;span class=&quot;c&quot;&gt;# start the web server, default port is 8080&lt;/span&gt;
airflow webserver &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; 8080

&lt;span class=&quot;c&quot;&gt;# start the scheduler&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# open a new terminal or else run webserver with ``-D`` option to run it as a daemon&lt;/span&gt;
airflow scheduler

&lt;span class=&quot;c&quot;&gt;# visit localhost:8080 in the browser and use the admin account you just&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# created to login. Enable the example_bash_operator dag in the home page&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the installation is complete, I ensured to make all necessary changes
to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;airflow.cfg&lt;/code&gt; located at ~/airflow/ (which in my case was /home/pi/airflow/).
The primary setting I needed to know was where my DAGs where located. This setting
is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dags_folder = /home/pi/airflow/dags&lt;/code&gt; in my case. This is where my DAG (&lt;strong&gt;.py&lt;/strong&gt; files
will go). I also did not adjust the &lt;strong&gt;default_timezone&lt;/strong&gt;. I used &lt;strong&gt;utc&lt;/strong&gt; time as I did 
for my PostgreSQL instance and as I would recommend anyone reading this should do.
Timezones have caused me some real headaches over the years and I find the best practice
to be, storing all data using UTC timestamps and then handling the timezones on the
application end if necessary.&lt;/p&gt;

&lt;h3 id=&quot;airflow-dag-creation&quot;&gt;Airflow: DAG creation&lt;/h3&gt;

&lt;p&gt;A DAG is a Directed Acyclic Graph as I have already mentioned. Put simply, it is a graph
who’s nodes are tasks. Those tasks are connected from one node to the next and the
overall graph contains no closed loop. This screenshot of my DAG taken from the
Airflow webserver should clarify the &lt;em&gt;Graph&lt;/em&gt; concept. My tasks are the nodes and the overall
graph is the name of my DAG. Notice there is directionality illustrated by an arrow pointing
from &lt;strong&gt;load_pg_from_influx&lt;/strong&gt; :arrow_right: &lt;strong&gt;moving_average_calc&lt;/strong&gt;. Therefore, the moving average task will
be executed after the loading of data to postgres task is complete:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/airflow_graph.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is my DAG located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/pi/airflow/dags/&lt;/code&gt;. I used two operators for the two
different tasks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;dbt_operator - (from pkg &lt;strong&gt;airflow-dbt&lt;/strong&gt; in PyPi) which makes it easy to execute my dbt model&lt;/li&gt;
  &lt;li&gt;BashOperator - which allows me to execute commands in Bash shell&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I have set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schedule_interval&lt;/code&gt; to run at the top of each hour, every hour of every day. This
should be familiar to those that have scheduled a cronjob before… I can use the same syntax.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow_dbt.operators.dbt_operator&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DbtRunOperator&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.bash&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.utils.dates&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days_ago&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# The default dir contains my dbt models, the retries has been added
# to handle if the cloud provider is offline for maintenance (this happened).
# Finally, I allow 30 minutes to attempt a re-execution of the DAG
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;dir&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;/home/pi/dbt_world/health_metrics&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;start_date&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days_ago&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;retries&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;retry_delay&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minutes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dag_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;pulse_ox_data&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schedule_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;0 * * * *&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;influx_pg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;load_pg_from_influx&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bash_command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;/usr/bin/python3 /home/pi/pg_loader/health_metrics_loader.py&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dbt_run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DbtRunOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;moving_average_calc&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;influx_pg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dbt_run&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please note the task variable influx_pg pipes into the dbt_run variable in the above script/DAG. This defines my
graph directionality and order of execution. If I decide at some later point in time that
I would like to materialize another view or table from the original influx_pg data, I can
pipe into a list object (e.g. &lt;strong&gt;t1 » [t2, t3]&lt;/strong&gt;). Like I said, what I have done so far
is not very complex. Despite not being very complex, I was able to solve my scheduling dilemma
in an elegant manner. If this were some critical data pipeline, I could send emails to myself
or my team if my tasks fail. Airflow gives much more information and ability than a vanilla cronjob
would to allow for troubleshooting efficiencies, bottlenecks or failures.&lt;/p&gt;

&lt;p&gt;Last but not least, here is a screenshot of the resulting moving average (blue) for my daughter’s heartrate
overlayed onto the original heartrate data stored in my PostgreSQL database (orange):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ma_bpm.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If someone reads this and spots an obvious mistake or has some additional insights or questions,
please send me an email.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I don’t know what is behind the curtain; only that I need to find out.”
― Richard Paul Evans, Lost December&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="rpi" /><category term="postgres" /><category term="dbt" /><category term="airflow" /><summary type="html">My previous two posts involved installing a postgres server on a cheap, spare raspberry pi 3b+. The motivation was to save my time-series data for longer than 30-days since my free InfluxDB Cloud account, only has a 30-day retention policy.</summary></entry><entry><title type="html">Extract, transform &amp;amp; load: InfluxDB Cloud to Local PostgreSQL (Part 2)</title><link href="http://localhost:4000/rpi/influxdb/postgresql/etl/cronjob/2021/07/12/influx_to_postgres.html" rel="alternate" type="text/html" title="Extract, transform &amp;amp; load: InfluxDB Cloud to Local PostgreSQL (Part 2)" /><published>2021-07-12T17:00:00+02:00</published><updated>2021-07-12T17:00:00+02:00</updated><id>http://localhost:4000/rpi/influxdb/postgresql/etl/cronjob/2021/07/12/influx_to_postgres</id><content type="html" xml:base="http://localhost:4000/rpi/influxdb/postgresql/etl/cronjob/2021/07/12/influx_to_postgres.html">&lt;h3 id=&quot;quick-recap&quot;&gt;Quick Recap&lt;/h3&gt;

&lt;p&gt;During part 1, I walked through the following actions in the effort
to keep my cloud storage data for a longer duration:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Burning a new OS image to a microSD&lt;/li&gt;
  &lt;li&gt;Setting up a raspberry pi 3b+ to run headless&lt;/li&gt;
  &lt;li&gt;Installing and configuring a postgresql server on the rpi&lt;/li&gt;
  &lt;li&gt;Testing the postgresql instance by writing data to it remotely&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This probably sounds backwards, if you did not read 
&lt;a href=&quot;https://soitgoes511.github.io/rpi/headless/postgresql/2021/07/10/rpi_headless_postgres.html&quot;&gt;part 1&lt;/a&gt;, 
but to recap, I don’t pay for my InfluxDB cloud account and have a 30-day 
retention policy on my data storage. 30-days is fine for real-time
monitoring but for longer term modeling, I have installed a postgreSQL
database on an oldish raspberry pi 3b+ and aim to collect longintudinal 
health metrics. So, without further ado…&lt;/p&gt;

&lt;h3 id=&quot;extract-transform-and-load&quot;&gt;Extract, Transform and Load&lt;/h3&gt;

&lt;p&gt;First, I would like to offer an explanation as to why I have decided to not store the data 
on a local instance of InfluxDB and then I will give some insight into the data I have stored 
on the cloud, the structure of the data and the resolution. Finally, I will share the script
I will be using to perform the ETL operation.&lt;/p&gt;

&lt;h4 id=&quot;why-postgres-rather-than-a-local-influxdb-instance&quot;&gt;Why postgres rather than a local influxdb instance?&lt;/h4&gt;

&lt;p&gt;Using InfluxDB OSS, seemed like the most obvious choice. This was my initial intent. What
stopped me short of doing this is the lack of an InfluxDB version &amp;gt;=2.0 available for my
rpi3B+ architecture (armv7l). If I had a spare rpi4 lying around, this would have been
my choice. I do have InfluxDB version 1.8.6-1 available in my repo:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;influxdb/unknown,now 1.8.6-1 armhf
  Distributed time-series database.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This version will allow me to query with both the legacy InfluxQL query language and
Flux if I toggle that option within the configuration. But, I cannot write to the
time-series database using Flux. I do not like the idea of being locked into the
older version of InfluxDB and therefore, I chose postgres as my storage solution.&lt;/p&gt;

&lt;h4 id=&quot;a-glimpse-of-the-raw-data&quot;&gt;A glimpse of the raw data&lt;/h4&gt;

&lt;p&gt;The screenshot below was taken from InfluxData’s GUI &lt;strong&gt;Data Explorer&lt;/strong&gt;. A few items to note. I did not
aggregate the data. I take samples from my daughter’s pulse oximeter once a second. This is the same
resolution I store in the cloud. The start time is the starting timeframe of the query (now() - 10s) in this
case. The stop time is/was now(). The time column is the actual time of the sensor read. The value
column is the sensor reading itself and is paired with the field column. The field in time-series lingo is
an un-indexed column. The measurement column is analogous to the table name in a relational database which in this
case is spo2 (not the best choice and I should have named it differently). Finally, UID is a tag or an
indexed column. This is residual from me experimenting with multiple sensors and completely lacks
any information or utility at this point-in-time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/streaming_tables.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sensor reads are all stored to InfluxDB in UTC time. I plan on doing the same for my postgresql instance.
From my experience, timestamps can cause some real headaches and the clearest path is to store in UTC time
and to handle any timezones on the application end while modeling.&lt;/p&gt;

&lt;h4 id=&quot;etl-script&quot;&gt;ETL Script&lt;/h4&gt;

&lt;p&gt;So, a brief outline of what the script should do:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a time range/constraint of one hour since I will be taking snapshots every hour of everyday&lt;/li&gt;
  &lt;li&gt;Query InfluxDB using those created time constraints&lt;/li&gt;
  &lt;li&gt;Handle as much of the transformation as I can during the query, e.g. Aggregate, Pivot and Drop columns&lt;/li&gt;
  &lt;li&gt;Save query return to pandas dataframe&lt;/li&gt;
  &lt;li&gt;Drop final un-wanted columns, convert timestamp into datetime64 and make dttm an index&lt;/li&gt;
  &lt;li&gt;Push dataframe to postgresql instance, appending onto table if it exists&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This script will only execute once, therefore, since I am taking one hour snapshots, I need
to schedule the script to execute once per hour. More on that shortly.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Filename: health_metrics_loader.py
# Author:   Mike Hinkle
# Purpose:  Extract data from InfluxDB cloud account, sanitize 
#           and push to local postgres instance
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;influxdb_client&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InfluxDBClient&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dotenv&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_dotenv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;load_dotenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# Load secrets from dotenv file
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TOKEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TOKEN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ORG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ORG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CONNECTION&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PG_CONNECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PG_CONNECT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# Create time range to bound query
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utcnow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%Y-%m-%dT%H:00:00Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utcnow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hours&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;last_hour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%Y-%m-%dT%H:00:00Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# Instantiate connection object for TSDB
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InfluxDBClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TOKEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;org&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;query_api&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# Query InfluxDB cloud instance and return in df
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;from(bucket: &quot;pulse_oximeter&quot;) &apos;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;|&amp;gt; range(start: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_hour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;, stop: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;) &quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;|&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &quot;spo2&quot;) &apos;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;|&amp;gt; aggregateWindow(every: 5s, fn: mean, createEmpty: false)&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;|&amp;gt; drop(columns: [&quot;_start&quot;,&quot;_stop&quot;,&quot;_measurement&quot;,&quot;uid&quot;])&apos;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;|&amp;gt; pivot(rowKey:[&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)&apos;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#
# Close connection object
#
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__del__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#
# Try and except to handle case when cloud provider is offline or
# rpi unplugged (away from wifi).
#
# Drop don&apos;t care columns, rename time header and set time as index
#
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;table&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;result&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;datetime64[us]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Instantiate sqlalchemy connection object
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_CONNECT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Use pandas to write dataframe to postgres instance
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;df_current_sats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pox_five_second_mean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;append&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Close sqlalchemy connection object
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dispose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;KeyError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;scheduling-considerations&quot;&gt;Scheduling considerations&lt;/h3&gt;

&lt;p&gt;Now that the script is complete, and I know that it works, I need to
ensure that it is run once per hour. The path of least resistence since
I am running a single script on the same raspberry hosting the postgresql
server, will be to run the script as a cronjob. I realize that there
are more robust scheduling solutions such as &lt;strong&gt;Apache Airflow&lt;/strong&gt; and I
intend to do the same with a DAG (Directed Acyclic Graph) at a later date.
For now, I want to get the data collected and I do not have airflow
installed on the rpi3b+. So, forgive my rush and the simplicity of
the solution.&lt;/p&gt;

&lt;p&gt;After pushing the script to the raspberry pi 3b+, installing the
dependencies (python-dotenv, psycopg2, sqlalchemy, influxdb-client and
pandas) and creating my .env file containing my secrets, I can create the cronjob
like so:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi@influxdb-historic:~ &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;crontab &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/bin/python3 /home/pi/pg_loader/health_metrics_loader.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/cron.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That should do the trick. I will check later tonight, plug my new postgres instance
into my Grafana install running on my workstation and see if the data
is collecting successfully.&lt;/p&gt;

&lt;h3 id=&quot;8-hours-later-update&quot;&gt;8-hours later (Update)&lt;/h3&gt;

&lt;p&gt;It looks like the script and the cronjob were a success:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/grafana_historic.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I will be revisiting this project to discuss various models when I have ample
historical data. I might also redo the scheduling using Apache Airflow. I am hesitant
since it seems like bird hunting with a scud missile and slightly overkill.
If you see any opportunities for improvements or mistakes, please shoot me an 
email. Thank you for reading and for your time.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Questions give us no rest.”
― Ayn Rand&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="rpi" /><category term="influxdb" /><category term="postgresql" /><category term="etl" /><category term="cronjob" /><summary type="html">Quick Recap</summary></entry><entry><title type="html">RPI3b+ (Headless) running PostgreSQL instance on localhost for longer term data retention (Part 1)</title><link href="http://localhost:4000/rpi/headless/postgresql/2021/07/10/rpi_headless_postgres.html" rel="alternate" type="text/html" title="RPI3b+ (Headless) running PostgreSQL instance on localhost for longer term data retention (Part 1)" /><published>2021-07-10T17:00:00+02:00</published><updated>2021-07-10T17:00:00+02:00</updated><id>http://localhost:4000/rpi/headless/postgresql/2021/07/10/rpi_headless_postgres</id><content type="html" xml:base="http://localhost:4000/rpi/headless/postgresql/2021/07/10/rpi_headless_postgres.html">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;I have been collecting my daughter’s pulse oximeter data for almost two years. She is
24-hour ventilator dependent and my wife and I have had some close calls where she has stopped 
breathing. Initially, the only way we could see her sats was to be physically in front of the 
pulse oximeter. Obviously, this is not a realistic option. Over the last few years, I have
attemped different solutions to not only monitor her, but also to model her breathing and
various health metrics. Some of those solutions were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;php scripts through apache on a local server&lt;/li&gt;
  &lt;li&gt;InfluxDB also on a local server and plugged into Grafana for visualization&lt;/li&gt;
  &lt;li&gt;InfluxDB Cloud, which enabled us to monitor her remotely&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I still use number 3 above to this very day:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/mobile_web_grafana.jpg&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Full disclosure, I am cheap and use the hobbyist free account for both InfluxDB and Grafana which understandably 
comes with some limitations. One of those limitations is a 30-day data retention policy for InfluxDB. 30-days is 
great for real time monitoring, but what if I want to model seasonal patterns year-on-year? This is
the spirit of this project. I want to capture as much historical data as possible without spending
any money. I already have a spare raspberry pi 3b+ and I have an internet connection, what more do I need?&lt;/p&gt;

&lt;p&gt;Covered in this write-up today will be:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Downloading Raspberry Pi OS and flashing the OS to a microSD&lt;/li&gt;
  &lt;li&gt;Activating SSH and the WIFI connection to run headless&lt;/li&gt;
  &lt;li&gt;Remotely logging into pi to bring the system up-to-date&lt;/li&gt;
  &lt;li&gt;Downloading and configuring postgreSQL&lt;/li&gt;
  &lt;li&gt;Testing that configuration&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Part 2 of this series will actually extract the data I have in my InfluxDB Cloud instance using the Flux query language, 
shape the data and push the desired data to the postgreSQL instance I will be creating today. So, let us begin…&lt;/p&gt;

&lt;h3 id=&quot;downloading-raspberry-pi-os-and-flashing-the-os-to-a-microsd-card&quot;&gt;Downloading raspberry pi OS and flashing the OS to a microSD card&lt;/h3&gt;

&lt;p&gt;I have been using raspberry pi’s for years and I cannot emphasize enough, how great they are for the price. Once ordering
a board which typically costs me ~35$ US, I need to download an OS and flash that OS to a microSD card.
Since I am looking to do this quickly, I am not installing anything cute like Arch or Gentoo. Raspberry Pi OS it is. The images can be
downloaded here: &lt;a href=&quot;https://www.raspberrypi.org/software/operating-systems/&quot;&gt;OS Download&lt;/a&gt;. You will find multiple versions,
in my case since I am configuring this single board computer to run headless, I do not need a GUI or any additional
software (like LibreOffice, etc..) so I selected Raspberry Pi OS Lite.&lt;/p&gt;

&lt;p&gt;Once the OS is downloaded, I change into the directory where the download was saved (in my case ~/Downloads/) and I unzip the 
image using the following command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ unzip 2021-05-07-raspios-buster-armhf-lite.zip&lt;/code&gt; which will uncompress a single
image. In  my case, the image was named &lt;strong&gt;2021-05-07-raspios-buster-armhf-lite.img&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;At this stage, for the past few years, I have used balenaEtcher to flash images to microSDs: &lt;a href=&quot;https://www.balena.io/etcher/&quot;&gt;Balena Download&lt;/a&gt;,
I have never had any issues and the software works wonderfully. Today however, I wanted to try out a USB flashed I have resident
on my OS which comes installed on POP!_OS named &lt;strong&gt;Popsicle&lt;/strong&gt;.:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/popsicle_screenshot.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the image was selected and pointed at my microSD card, I can proceed to burn the image. If I was hooking the raspberry pi
up to a monitor and keyboard after the image was burned, I could go ahead and install the microSD card into my raspberry pi
and skip the next section below. But, since I do not have an extra monitor and want to run headless, I will be adding a
few extra files to the microSD card before I remove the microSD card from my workstation.&lt;/p&gt;

&lt;h3 id=&quot;activating-ssh-and-wifi-connection-to-run-headless&quot;&gt;Activating SSH and WIFI connection to run headless&lt;/h3&gt;

&lt;p&gt;To avoid needing a monitor and a keyboard for my rpi and to make the single board computer available online to access
via SSH, I need to add 2 files to the root of /boot on the newly installed image. On my workstation, once the microSD is
mounted to my filesystem, I can change directory into /boot (the microSD card is mounted for me at /media/run/),
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch ssh&lt;/code&gt;, this will create an empty file named &lt;strong&gt;ssh&lt;/strong&gt; which enables ssh. The second file I need to create will be named &lt;strong&gt;wpa_supplicant.conf&lt;/strong&gt; and it should be located
in the same directory, I just created the empty ssh file. The contents of this file are:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;country=FR
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1

network={
    ssid=&quot;NAME-OF-YOUR-WIFI-NETWORK&quot;
    psk=&quot;PASSWORD-FOR-YOUR-WIFI&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can save and close the above file and I am done. The microSD card can be unmounted, ejected and installed
into my raspberry pi. Once the raspberry pi is plugged into a 5V power supply, and I am within range
of my wifi, I should be able to ssh in using my workstation.&lt;/p&gt;

&lt;h3 id=&quot;remotely-logging-into-pi-to-bring-the-system-up-to-date&quot;&gt;Remotely logging into pi to bring the system up-to-date&lt;/h3&gt;

&lt;p&gt;Once my pi is plugged in and out of the way, I can use &lt;strong&gt;nmap&lt;/strong&gt; from my workstation to find out
which ip address my rpi was assigned. But, first I need my inet ip address:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ifconfig

wlp5s0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1500
    inet 192.168.0.28  netmask 255.255.255.0  broadcast 192.168.0.255
    inet6 2a01:e0a:897:1680:4e48:5fa5:da96:558c  prefixlen 64  scopeid 0x0&amp;lt;global&amp;gt;
    inet6 2a01:e0a:897:1680:19f8:90c1:6834:f9e6  prefixlen 64  scopeid 0x0&amp;lt;global&amp;gt;
    inet6 fe80::3e7d:52b9:f37d:b024  prefixlen 64  scopeid 0x20&amp;lt;link&amp;gt;
    ether 74:d8:3e:01:6d:14  txqueuelen 1000  (Ethernet)
    RX packets 66847  bytes 50766070 (50.7 MB)
    RX errors 0  dropped 36  overruns 0  frame 0
    TX packets 46905  bytes 9180586 (9.1 MB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The relevant ip address above is &lt;strong&gt;192.168.0.28&lt;/strong&gt;. Armed with this, I can now use
nmap to determine my rpi’s address:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo nmap -sn 192.168.0.28/24

Starting Nmap 7.80 ( https://nmap.org ) at 2021-07-10 14:58 CEST
Nmap scan report for 192.168.0.12 
Host is up (0.25s latency).
MAC Address: B8:27:EB:E0:08:FB (Raspberry Pi Foundation)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, I can access my rpi remotely via ssh using the ip address &lt;strong&gt;192.168.0.12&lt;/strong&gt;. 
Please note that the default password for raspberry’s is &lt;em&gt;raspberry&lt;/em&gt; so enter that
when prompted:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh pi@192.168.0.12

pi@192.168.0.12&apos;s password: 
Linux influxdb-historic 5.10.17-v7+ #1421 SMP Thu May 27 13:59:01 BST 2021 armv7l

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Sat Jul 10 12:27:06 2021 from 192.168.0.28
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please note that I have logged in already prior to this so your output my be slightly different.
The first thing I did was to change the password from raspberry to a password of my choice. To
do this, type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passwd&lt;/code&gt; and then type enter. You could also type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passwd pi&lt;/code&gt; and then enter. 
Follow the prompts to update the password.&lt;/p&gt;

&lt;p&gt;After this is complete, I like to change the keyboard and language settings using
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo raspi-config&lt;/code&gt;. Once all the settings are to my liking, I save and restart the pi:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo reboot&lt;/code&gt;. That will kick me off ssh and terminate my connection. After a minute or
so, I can re-connect via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh pi@192.168.0.12&lt;/code&gt;, enter my new password and once logged in,
continue to update my packages from the repo with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt update &amp;amp;&amp;amp; sudo apt upgrade&lt;/code&gt;.
That’s it, for this section, next step will be downloading and configuring the postgresql
server.&lt;/p&gt;

&lt;h3 id=&quot;downloading-and-configuring-the-postgresql-database-server&quot;&gt;Downloading and configuring the PostgreSQL Database Server&lt;/h3&gt;

&lt;p&gt;If you are un-familiar with databases, PostgreSQL is considered a RDBMS or
&lt;strong&gt;R&lt;/strong&gt;elational &lt;strong&gt;D&lt;/strong&gt;atabase &lt;strong&gt;M&lt;/strong&gt;anagement &lt;strong&gt;S&lt;/strong&gt;ystem and is in a nutshell, an excellent place to store 
relational data. In a corporate setting I was more familiar with Oracle, but PostgreSQL is essentially 
the same thing for zero cost (which I would argue makes it better than Oracle :stuck_out_tongue_winking_eye:).
Anyway, moving on… Let’s install it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ apt search postgresql

postgresql/stable,now 11+200+deb10u4 all
  object-relational SQL database (supported version)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above command is used to search through the default system repos. There will be many items returned,
but the package I would like to install is the supported version seen above.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt install postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this is complete, you can check to see if the database server is running:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ systemctl status postgresql

● postgresql.service - PostgreSQL RDBMS
   Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor preset: enabled)
   Active: active (exited) since Sat 2021-07-10 12:50:01 CEST; 2h 57min ago
  Process: 7771 ExecStart=/bin/true (code=exited, status=0/SUCCESS)
 Main PID: 7771 (code=exited, status=0/SUCCESS)

Jul 10 12:50:01 influxdb-historic systemd[1]: Starting PostgreSQL RDBMS...
Jul 10 12:50:01 influxdb-historic systemd[1]: Started PostgreSQL RDBMS.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I didn’t see this above, I would need to start the service myself. Also, if I would like
the postgres database to start at boot, then I would need to enable it (assuming systemd):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl start postgresql
$ sudo systemctl enable postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, now postgreSQL is installed and running in the background. I can login into the database
but need to switch to the postgres user first:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo su - postgres
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now to connect through CLI using the postgresql-client:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgres@influxdb-historic:~$ psql
psql (11.12 (Raspbian 11.12-0+deb10u1))
Type &quot;help&quot; for help.

postgres=#
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And it was a success. I typically create a new user at this point with a password. This
user I am creating will be the owner of my historical data database.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgres=# CREATE USER soitgoes511 WITH PASSWORD &apos;&amp;lt;YOUR_PASSWORD_HERE&amp;gt;&apos;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, I want to create my database and change the ownership to the new user I created:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgres=# CREATE DATABASE pulse_oximeter_historic;
postgres=# ALTER DATABASE pulse_oximeter_historic OWNER TO soitgoes511;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can see my new database owned by yours truly:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgres=# \l
                                          List of databases
          Name           |    Owner    | Encoding |   Collate   |    Ctype    |   Access privileges   
-------------------------+-------------+----------+-------------+-------------+-----------------------
 postgres                | postgres    | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 pulse_oximeter_historic | soitgoes511 | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 template0               | postgres    | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
                         |             |          |             |             | postgres=CTc/postgres
 template1               | postgres    | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
                         |             |          |             |             | postgres=CTc/postgres
(4 rows)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test-database-by-writing-to-it-remotely&quot;&gt;Test database by writing to it remotely&lt;/h3&gt;

&lt;p&gt;Before testing, I need to make a a few changes to some of the postgreSQL configuration files. The loader I will
be writing to populate this database with historical data will most likely run locally (on the pi), but I will be testing
from my workstation. Therefore, I need to give authorization for my ip address to connect to the db and I
need to the database to listen for more than the localhost. I will also be accessing the database to query data for modeling
eventually and will need to ensure that I can access it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo vim /etc/postgresql/11/main/postgresql.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First, I uncomment and change this line under &lt;em&gt;connections and authentication&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;listen_addresses = &apos;*&apos;                  # what IP address(es) to listen on;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Save and close. Then I can open up &lt;strong&gt;pg_hba.conf&lt;/strong&gt; and give permissions to my workstation to connect:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo vim /etc/postgresql/11/main/pg_hba.conf

host    all             all             192.168.0.28/32         trust
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this line is appended, I can restart my postgresql server and attempt to write some data to it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl restart postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From my workstation, I am using a &lt;strong&gt;jupyter-notebook&lt;/strong&gt; and python 3 to first test the remote connection and
then to load a dummy dataset I downloaded as a csv file off &lt;strong&gt;Kaggle&lt;/strong&gt;. Here is a screenshot of those scripts
and the output:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;psycopg2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;192.168.0.12&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pulse_oximeter_historic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;soitgoes511&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;YOUR_PSQL_PASSWORD&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create a cursor
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# execute a statement
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PostgreSQL database version:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT version()&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# display the PostgreSQL database server version
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# close the communication with the PostgreSQL
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The connection was successful. OUTPUT:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PostgreSQL database version:
(&apos;PostgreSQL 11.12 (Raspbian 11.12-0+deb10u1) on arm-unknown-linux-gnueabihf, compiled by gcc (Raspbian 8.3.0-6+rpi1) 8.3.0, 32-bit&apos;,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let me attempt to write some data to the db:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;postgresql://soitgoes511:&amp;lt;YOUR_PSQL_PASSWORD&amp;gt;@192.168.0.12:5432/pulse_oximeter_historic&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_best_sellers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/home/soitgoes/Kaggle/bestsellers_with_categories.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_best_sellers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bestsellers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;replace&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dispose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, to sanity check the data made it there, let me query it:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;postgresql://soitgoes511:&amp;lt;YOUR_PSQL_PASSWORD&amp;gt;@192.168.0.12:5432/pulse_oximeter_historic&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;SELECT * FROM bestsellers LIMIT 5;&quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dispose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dummy_table.png&quot; alt=&quot;drawing&quot; style=&quot;max-width: 100%; height: auto; text-align: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It worked :sparkles:. That is a wrap for today. Part II as I mentioned will delve into actually extracting the relevant
data, transforming/shaping it and then loading it into my new postgres instance hosted on my very cheap and
wonderful rpi3b+. Thank you for reading.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I think, at a child’s birth, if a mother could ask a fairy godmother to endow it with the most useful gift, that gift would be curiosity.”
― Eleanor Roosevelt&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="rpi" /><category term="headless" /><category term="postgresql" /><summary type="html">Motivation</summary></entry><entry><title type="html">Broken virtual environments: Why it happened and how I should have prevented it</title><link href="http://localhost:4000/linux/update/python/venv/broken/prevention/2021/07/06/fixing_broken_venv.html" rel="alternate" type="text/html" title="Broken virtual environments: Why it happened and how I should have prevented it" /><published>2021-07-06T17:00:00+02:00</published><updated>2021-07-06T17:00:00+02:00</updated><id>http://localhost:4000/linux/update/python/venv/broken/prevention/2021/07/06/fixing_broken_venv</id><content type="html" xml:base="http://localhost:4000/linux/update/python/venv/broken/prevention/2021/07/06/fixing_broken_venv.html">&lt;p&gt;Approximately 3-4 days ago, I updated my Linux box (&lt;strong&gt;Pop!_OS&lt;/strong&gt; version 20.10 -&amp;gt; 21.04). 
I have run Linux long enough to know that there could be unintended issues during
these major version updates. My first defense against possible headaches is to wait for
a few days and hope others find the bugs, report them and everything gets ironed out quickly.
Years ago I was not as cautious :see_no_evil:, but I no longer have the luxury of time to bang my head
into a desk for days and slog through issues or re-compile kernels. I just play it safe now.&lt;/p&gt;

&lt;p&gt;The good news is that despite a drastic change to the Pop!_OS desktop environment
(from &lt;strong&gt;GNOME 3.38.4&lt;/strong&gt; to &lt;strong&gt;COSMIC&lt;/strong&gt;), the update and upgrade went very smoothly. The new DE
is an improvement on GNOME, my system is very responsive, the stars all aligned and
everything is perfect. Kudos to the system76 team :thumbsup:. But.. Not everything
was perfect…&lt;/p&gt;

&lt;h3 id=&quot;what-exactly-was-the-problem&quot;&gt;What exactly was the problem?&lt;/h3&gt;

&lt;p&gt;Let me begin by saying that my issue has nothing to do with my updated OS. I decided
yesterday that I wanted to update a Heroku application which lives in a local repository
on my now updated workstation. This application was written in Python and the interpreter executed within a 
virtual environment (venv). Virtual environments should be self contained, correct? 
Sounds safe right :skull:? What happened was that the OS upgrade replaced my pre-existing system python (3.8) with
a newer python version (3.9). Had I sourced a standalone python install when creating the
virtual environment, or had the OS just switched PYTHON_TARGETS, then I would not be
writing this right now. I had created the venv like so for this particular Heroku application:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;soitgoes@pop-os:~$ python3 -m venv venv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The above command created said virtual environment inside a directory called venv with a symbolic link
pointing from the venv interpreter to my system python interpreter @ /usr/bin/python3.8 which no longer existed :disappointed::&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lrwxrwxrwx 1 soitgoes soitgoes    6 Jun 17 21:38 python3.8 -&amp;gt; python&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;At my previous work, I had many python versions installed on a dev mount to avoid this very issue.&lt;/p&gt;

&lt;h3 id=&quot;how-to-fix-the-issue-and-my-steps-moving-forward-&quot;&gt;How to fix the issue and my steps moving forward …&lt;/h3&gt;

&lt;p&gt;Install a standalone development python which lives seperately and isolated from my system python. This
would avoid the pain I am enduring now when my system is updated again with a new python install. 
I have no excuse for dropping the ball like this but it is what it is. Lesson learned.
Steps to remedy the situation, starting with that isolated python install:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download desired python version &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ wget https://www.python.org/ftp/python/3.8.11/Python-3&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Uncompress the python version   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ tar -xzvf Python-3.8.11.tgz&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Change into directory           &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ cd Python-3.8.11/&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Configure with target location  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ ./configure --prefix=/home/soitgoes/python-3.8.11&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Compile and build               &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ make&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Test build                      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ make test&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Install python to target location  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ make altinstall&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I can see that the above steps were successful by executing the newly installed
interpreter which was specified in my target location above (/home/soitgoes/python-3.8.11)
:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;soitgoes@pop-os:~$ ~/python-3.8.11/bin/python3.8&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And voilà… The REPL (&lt;strong&gt;R&lt;/strong&gt;ead, &lt;strong&gt;E&lt;/strong&gt;valuate, &lt;strong&gt;P&lt;/strong&gt;rint, &lt;strong&gt;L&lt;/strong&gt;oop) prompt appears reflecting
my desired version:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Python 3.8.11 (default, Jul  6 2021, 22:44:39)
[GCC 10.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My system python has been unchanged and I can verify this very easily:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;soitgoes@pop-os:~$ which python3
/usr/bin/python3
soitgoes@pop-os:~$ python3
Python 3.9.5 (default, May 11 2021, 08:20:37)
[GCC 10.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please notice the versioning differences. The next step in this process will be to
rebuild the symbolic links in the affected virtual environment. Please make note that
the python3 symlink points to the system /usr/bin/python3 (which is the issue since it
is now python 3.9 rather than 3.8). The other symlinks just chain all the python 
aliases together: python -&amp;gt; python3 which I just mentioned points to the system python
install. Finally, the last link is python3.9 -&amp;gt; python3 again:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lrwxrwxrwx 1 soitgoes soitgoes    7 Jul  4 22:58 python -&amp;gt; python3
lrwxrwxrwx 1 soitgoes soitgoes   16 Jul  4 22:58 python3 -&amp;gt; /usr/bin/python3
lrwxrwxrwx 1 soitgoes soitgoes    7 Jul  4 22:58 python3.9 -&amp;gt; python3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These soft links need to be re-directed to my newly built and installed python which
is now located in my home directory. I can do this simply by changing into the affected
venv/bin/ directory where the symlinks are present and then:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ ln -sf /home/soitgoes/python-3.8.11/bin/python3.8 python3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The symlinks are now corrected. When activating the previously affected venv, and
running the python interpreter I am greeted with version 3.8.11 rather than 3.9.5.
My application will again run without the need of rebuilding all of my dependencies.
Since this is fixed and it is getting late, I will need to actually do what I set out
to do initially (update my Heroku app) later. Enough problems solved for one day.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The moral of the story is to ensure you seperate your system dependencies from your
development dependencies. No sense in muddying the waters and causing unnecessary
headaches.&lt;/strong&gt; :heavy_check_mark:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The mind is not a vessel to be filled, but a fire to be kindled.
&lt;em&gt;― Plutarch&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="linux" /><category term="update" /><category term="python" /><category term="venv" /><category term="broken" /><category term="prevention" /><summary type="html">Approximately 3-4 days ago, I updated my Linux box (Pop!_OS version 20.10 -&amp;gt; 21.04). I have run Linux long enough to know that there could be unintended issues during these major version updates. My first defense against possible headaches is to wait for a few days and hope others find the bugs, report them and everything gets ironed out quickly. Years ago I was not as cautious :see_no_evil:, but I no longer have the luxury of time to bang my head into a desk for days and slog through issues or re-compile kernels. I just play it safe now.</summary></entry><entry><title type="html">Initial observations while settling in</title><link href="http://localhost:4000/france/blog/careers/ee/2021/06/26/initial_observations.html" rel="alternate" type="text/html" title="Initial observations while settling in" /><published>2021-06-26T17:00:00+02:00</published><updated>2021-06-26T17:00:00+02:00</updated><id>http://localhost:4000/france/blog/careers/ee/2021/06/26/initial_observations</id><content type="html" xml:base="http://localhost:4000/france/blog/careers/ee/2021/06/26/initial_observations.html">&lt;p&gt;I have begun applying for jobs. Perhaps I am being un-realistic considering
I am a beginner in the French language. I could not imagine finding a job in
the states and not speaking English. :crossed_fingers:&lt;/p&gt;

&lt;h3 id=&quot;some-observations-&quot;&gt;Some observations …&lt;/h3&gt;

&lt;p&gt;My first observation is that most positions in the fields I am looking for are
looking for Master’s or PHd’s. I guess when university doesn’t cost $100k for
an undergraduate degree :moneybag:, there is more motivation to go a step further. I find myself
regretting not sticking it out for my Master’s but that ship has sailed. At least
I have experience in industry.&lt;/p&gt;

&lt;p&gt;My second observation is that Electrical Engineering degrees here do not translate
to the same studies in the US. I could be wrong and I will have a better perspective
on this very shortly. I have noticed quite a bit of variation in EE degrees in the US, 
too. I have friends which never studied transistors at their schools. We were drowned in
small signal models, large signal models, BJT’s, MOSFET’s, etc.. After long
discussions with my wife in the past, my understanding is that the French University
system is, for the most part, standardized.&lt;/p&gt;

&lt;p&gt;Due to the second observation above :point_up:, there will probably be some
ambiguity for potential employers. I am attempting to find a career in big data,
data science, data engineering, analytics, web development, etc.. Not sure if that
aligns even remotely with what their EE’s learn at University, but it is not a huge
leap from my past curriculum and studies. Only time will tell. It has only been
2 weeks since I set foot on French soil and I need to be patient and keep plugging away.
On the bright side, in the meantime, I can spend time with my family. I never would 
have had this much time with my children had we stayed in the US.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Per aspera ad astra&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Mike Hinkle</name></author><category term="france" /><category term="blog" /><category term="careers" /><category term="EE" /><summary type="html">I have begun applying for jobs. Perhaps I am being un-realistic considering I am a beginner in the French language. I could not imagine finding a job in the states and not speaking English. :crossed_fingers:</summary></entry><entry><title type="html">A new start in France</title><link href="http://localhost:4000/first/france/blog/careers/family/2021/06/21/new_start_in_france.html" rel="alternate" type="text/html" title="A new start in France" /><published>2021-06-21T16:01:00+02:00</published><updated>2021-06-21T16:01:00+02:00</updated><id>http://localhost:4000/first/france/blog/careers/family/2021/06/21/new_start_in_france</id><content type="html" xml:base="http://localhost:4000/first/france/blog/careers/family/2021/06/21/new_start_in_france.html">&lt;p&gt;I will keep this short and sweet. I left corporate life in &lt;strong&gt;America&lt;/strong&gt; to move to my wife’s home country, &lt;strong&gt;France&lt;/strong&gt;. I had worked in the &lt;em&gt;semiconductor industry&lt;/em&gt; for about 4 years as 
an &lt;em&gt;engineer&lt;/em&gt; in various roles. My background at University was &lt;strong&gt;Electrical Engineering&lt;/strong&gt; with a concentration in microelectronics. My employer treated me well and finances were good. 
I had a real career. About 3.5 years ago my beautiful daughter was born. My wife and I were informed at our 20 week sonogram that there were issues. Our life has never been the same 
since.&lt;/p&gt;

&lt;p&gt;I have been through plenty of hard times in my life, but to this day, seeing her struggles has really darkened my soul. We came to France so her and my son could be with family. We
came to France so they could be loved. We are only here on this earth for a brief moment and like &lt;em&gt;Bob Marley&lt;/em&gt; said so wonderfully:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…life is worth much more than gold.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is my first blog entry. I promise that I will delve into some fun projects down the road. I am waiting until we purchase our house to nerd out on some &lt;strong&gt;IoT and pub-sub messaging
systems&lt;/strong&gt;. If you read this, thank you. :grin:&lt;/p&gt;

&lt;div style=&quot;display: inline-block;&quot;&gt;
  &lt;img src=&quot;/assets/baby_girl_1.jpeg&quot; alt=&quot;drawing&quot; style=&quot;height: 300px;&quot; /&gt;
&lt;/div&gt;
&lt;div style=&quot;display: inline-block;&quot;&gt;
  &lt;img src=&quot;/assets/french_town_1.jpg&quot; alt=&quot;drawing&quot; style=&quot;height: 300px;&quot; /&gt;
&lt;/div&gt;
&lt;div style=&quot;display: inline-block;&quot;&gt;
  &lt;img src=&quot;/assets/baby_boy_1.jpeg&quot; alt=&quot;drawing&quot; style=&quot;height: 300px;&quot; /&gt;
&lt;/div&gt;</content><author><name>Mike Hinkle</name></author><category term="first" /><category term="france" /><category term="blog" /><category term="careers" /><category term="family" /><summary type="html">I will keep this short and sweet. I left corporate life in America to move to my wife’s home country, France. I had worked in the semiconductor industry for about 4 years as an engineer in various roles. My background at University was Electrical Engineering with a concentration in microelectronics. My employer treated me well and finances were good. I had a real career. About 3.5 years ago my beautiful daughter was born. My wife and I were informed at our 20 week sonogram that there were issues. Our life has never been the same since.</summary></entry></feed>